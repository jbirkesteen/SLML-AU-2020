{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 - Probability Distributions 3 and Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning contents:\n",
    "\n",
    "1. Histogram-based density estimation\n",
    "    - Display histogram densities\n",
    "2. Kernel density estimation\n",
    "    - Hypercube Kernel function\n",
    "    - Gaussian Kernel function\n",
    "3. K-Nearest Neigbours classification\n",
    "    - Generate data\n",
    "    - Classification function\n",
    "    - Display results\n",
    "4. K-Means clustering\n",
    "    - Display results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from math import sqrt\n",
    "from collections import Counter\n",
    "from scipy.stats import norm\n",
    "from sklearn import datasets\n",
    "from IPython.display import HTML\n",
    "\n",
    "import seaborn as sns; sns.set(); sns.set_palette('bright')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_1D(size, means, variances, pis):\n",
    "    result = 0\n",
    "    \n",
    "    for i, (mean, variance, pi) in enumerate(zip(means, variances, pis)):\n",
    "        result += pi * np.array(norm(mean, sqrt(variance)).rvs(size=size, random_state=26 + i))\n",
    "    \n",
    "    return result\n",
    "    \n",
    "means = [0.4, 2.0]\n",
    "variances = [0.2, 0.1]\n",
    "pis = [0.7, 0.3]\n",
    "data_1D = generate_data_1D(50, means, variances, pis)\n",
    "\n",
    "plt.scatter(data_1D, [0] * len(data_1D), alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Histogram-based density estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`histogram` takes `data`, bin size `delta` and returns `bins` (list of bins where `bin = list of points`) and their `probabilities`<br>\n",
    "The below implementation isn't very elegant, but it gets the job done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram(data, delta):\n",
    "    bin_point = min(data) + delta\n",
    "    bin_points = []\n",
    "    while bin_point < max(data):\n",
    "        bin_points.append(bin_point)\n",
    "        bin_point += delta\n",
    "    bin_points.append(bin_point)\n",
    "    data_sorted = sorted(data)\n",
    "    bins = []\n",
    "    for bin_i in bin_points:\n",
    "        current_bin = []\n",
    "        while data_sorted[0] < bin_i:\n",
    "            current_bin.append(data_sorted.pop(0))\n",
    "            if not data_sorted:\n",
    "                break\n",
    "        bins.append(current_bin)\n",
    "    N = len(data)\n",
    "    probabilities = [len(bin_i)/(N*delta) for bin_i in bins]\n",
    "    return bins, probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) Display histogram densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_histogram_density(data, delta):\n",
    "    bins, probabilities = histogram(data, delta)\n",
    "\n",
    "    plt.bar(range(len(bins)), list(map(lambda b: len(b), bins)))\n",
    "    plt.plot(range(len(probabilities)), probabilities, '-r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_histogram_density(data_1D, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_histogram_density(data_1D, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_histogram_density(data_1D, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_histogram_density(data_1D, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Kernel density estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Hypercube Kernel function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hypercube_kernel_function` takes `u` and returns 0 or 1 if `u` is inside 1/2 hypercube<br>\n",
    "Function defined as per slide 13 lecture 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypercube_kernel_function(u):\n",
    "    return 1 if np.linalg.norm(u) <= 1/2 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hypercube_kernel_density` takes any point `x`, data points `data`, size of a cube `h`, amount of dimensions `D` and returns probability density function based on Hypercube kernel function<br>\n",
    "Function defined as per slide 14 lecture 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypercube_kernel_density(x, data, h, D):\n",
    "    N = len(data)\n",
    "    return 1/N*sum([1/(h**D) * hypercube_kernel_function((x-x_n)/h) for x_n in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_hypercube_kernel_density_1D(data, h, color='b'):\n",
    "    xs = np.linspace(min(data), max(data), 200)\n",
    "    plt.plot(xs, list(map(lambda x: hypercube_kernel_density(x, data, h, 1), xs)), '-' + color, label='h=' + str(h))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_hypercube_kernel_density_1D(data_1D, 0.05, 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_hypercube_kernel_density_1D(data_1D, 0.2, 'b')\n",
    "display_hypercube_kernel_density_1D(data_1D, 1, 'g')\n",
    "display_hypercube_kernel_density_1D(data_1D, 2, 'm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Gaussian Kernel function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gaussian_kernel_function` takes pair of points `x` and `x_n`, size `h` and returns Gaussian kernel function for this pair of points<br>\n",
    "As the Gaussian kernel function is the Gaussian probability distribution function, we use norm.pdf imported from scipy.stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel_function(x, x_n, h):\n",
    "    return norm.pdf(x, loc=x_n, scale=h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gaussian_kernel_density` takes any point `x`, data points `data`, size `h` and returns Gaussian kernel density for point `x`<br>\n",
    "The idea here is the same as the hypercube kernel density, simply with another kernel function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel_density(x, data, h):\n",
    "    N = len(data)\n",
    "    return 1/N * sum([gaussian_kernel_function(x, x_n, h) for x_n in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_gaussian_kernel_density_1D(data, h, color='b'):\n",
    "    xs = np.linspace(min(data), max(data), 200)\n",
    "    plt.plot(xs, list(map(lambda x: gaussian_kernel_density(x, data, h), xs)), '-' + color, label='h=' + str(h))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_gaussian_kernel_density_1D(data_1D, 0.01, 'b')\n",
    "display_gaussian_kernel_density_1D(data_1D, 0.05, 'g')\n",
    "display_gaussian_kernel_density_1D(data_1D, 0.2, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) K-Nearest Neigbours classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris_x = np.array(iris.data[:, :2])  # we only take the first two features.\n",
    "iris_t = np.array(iris.target)\n",
    "\n",
    "def plot_iris(legend=True, classes=iris_t, target=plt):\n",
    "    scatter = target.scatter(iris_x[:, 0], iris_x[:, 1], c=classes, alpha=0.7, cmap='rainbow', edgecolor='none')\n",
    "    if legend:\n",
    "        legend = target.legend(*scatter.legend_elements(), loc=\"upper left\", title=\"Classes\")\n",
    "        return (scatter, legend)\n",
    "    return (scatter, )\n",
    "\n",
    "plot_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Classification function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`k_nearest_classification` takes any point `x`, data point positions `data_x`, their classes `data_t`, amount of neighbours `k` and returns class for the point `x`<br>\n",
    "Classification is done by sorting all points based on their distance from `x`. We keep the `k` closest points and use the Counter function to find the most frequent label out of these `k` neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest_classification(x, data_x, data_t, k):\n",
    "    data_pairs = zip(data_x, data_t)\n",
    "    k_nearest = sorted(data_pairs, key=lambda y: np.linalg.norm(y[0] - x))[:k]\n",
    "    dict_of_freq = Counter([label for point, label in k_nearest])\n",
    "    max_class = max(dict_of_freq, key=dict_of_freq.get)\n",
    "    return max_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3) Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mesh(pred_fn, n_class=3, x_min=4, x_max=8, y_min=2, y_max=4.5, target=plt):\n",
    "    h = 0.1  # step size in the mesh\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = np.array(list(map(lambda x: pred_fn(np.array(x)), np.c_[xx.ravel(), yy.ravel()])))\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = target.contourf(xx, yy, Z, alpha = 0.1, cmap=plt.cm.get_cmap('rainbow', n_class))\n",
    "    target.axis('tight')\n",
    "    if hasattr(target, 'xlim'):\n",
    "        target.xlim(x_min, x_max)\n",
    "        target.ylim(y_min, y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iris(False)\n",
    "plot_mesh(lambda x: k_nearest_classification(x, iris_x, iris_t, 1))\n",
    "plt.legend([], loc=\"upper left\", title=\"K=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iris(False)\n",
    "plot_mesh(lambda x: k_nearest_classification(x, iris_x, iris_t, 4))\n",
    "plt.legend([], loc=\"upper left\", title=\"K=4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iris(False)\n",
    "plot_mesh(lambda x: k_nearest_classification(x, iris_x, iris_t, 8))\n",
    "plt.legend([], loc=\"upper left\", title=\"K=8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) K-Means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`k_means_step` takes initial means `mus_0`, data points `data_x` and returns new means `mus` and `classes`<br>\n",
    "The function uses the ideas described in slide 4 to slide 7 of lecture 8. The function first calculates which of the $\\mu_k$ a given datapoint is closest to. Each class is then set as a key in a dictionary with its value being a list of the points belonging to it.  Equality 5 from slide 7 is then used to generate the new $\\mu_k$'s.<br>\n",
    "The function computes the new $\\mu_k$'s and which class each point belongs to without explicitly stating the onehot-vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_step(mus_0, data_x):\n",
    "    classes = []\n",
    "    for x in data_x:\n",
    "        distances = {class_k: np.linalg.norm(x - mu_k) for class_k, mu_k in enumerate(mus_0)}\n",
    "        classes.append(min(distances, key=distances.get))\n",
    "\n",
    "    class_dict = {}\n",
    "    for k, mu_k in enumerate(mus_0):\n",
    "        class_dict[k] = []\n",
    "        data_class = zip(data_x, classes)\n",
    "        for x, x_class in data_class:\n",
    "            if x_class == k:\n",
    "                class_dict[k].append(x)\n",
    "\n",
    "    for mu_k in class_dict:\n",
    "        class_dict[mu_k] = np.mean(np.vstack(class_dict[mu_k]), axis=0)\n",
    "    mus = [class_dict[mu_k] for mu_k in class_dict]\n",
    "    return mus, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`distortion_measure` takes means `mus`, `classes`, data points `data_x` and returns distortion of this classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distortion_measure(mus, classes, data_x):\n",
    "    \n",
    "    result = 0\n",
    "    \n",
    "    for i, c in enumerate(classes):\n",
    "        \n",
    "        x = data_x[i]\n",
    "        mu = mus[c]\n",
    "        distance = np.dot(np.array(mu) - np.array(x), np.array(mu) - np.array(x))\n",
    "        result += distance\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`optimize_k_means` takes initial means `mus_0`, data points `data_x` and callback `on_step`<br>\n",
    "The function only calls `on_step` if the newly computed $\\mu_k$ is lesser than the old $\\mu_k$(assuming that the distortion measure isn't already minimized before the very first call).\n",
    "\n",
    "`on_step` is a function that takes current `mus` and `classes` and should be called each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_k_means(mus_0, data_x, on_step):\n",
    "    mus_1, classes_1 = k_means_step(mus_0, data_x)\n",
    "    on_step(np.array(mus_1), np.array(classes_1))\n",
    "    distortion_1 = distortion_measure(mus_1, classes_1, data_x)\n",
    "    mus_2, classes_2 = k_means_step(mus_1, data_x)\n",
    "    distortion_2 = distortion_measure(mus_2, classes_2, data_x)\n",
    "    if distortion_2 < distortion_1:\n",
    "        on_step(np.array(mus_2), np.array(classes_2))\n",
    "        optimize_k_means(mus_2, data_x, on_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1) Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_k_means(mus, classes, target=plt):\n",
    "    plot = plot_iris(classes=classes, target=target)\n",
    "    scatter = target.scatter(mus[:, 0], mus[:, 1], c=[0, 1, 2], cmap='rainbow', marker='X', s=300, edgecolors='black')\n",
    "    return (*plot, scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "mus_0 = iris_x[:k]\n",
    "all_steps = []\n",
    "\n",
    "optimize_k_means(mus_0, iris_x, lambda mus, classes: all_steps.append((mus, classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_animation(all_steps, data_x):\n",
    "    \n",
    "    distortions = list(map(\n",
    "        lambda a: distortion_measure(a[0], a[1], data_x),\n",
    "        all_steps\n",
    "    ))\n",
    "    \n",
    "    fig, (ax, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
    "    \n",
    "    def animate(i):\n",
    "        ax.cla()\n",
    "        ax2.cla()\n",
    "        \n",
    "        plot1 = plot_k_means(all_steps[i][0], all_steps[i][1], target=ax)\n",
    "        ax2.plot(list(range(i)), distortions[:i], '-o')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Distortion')\n",
    "        return plot1\n",
    "    \n",
    "    anim = FuncAnimation(\n",
    "        fig, animate,\n",
    "        frames=len(all_steps), interval=500, blit=True\n",
    "    )\n",
    "    return HTML(anim.to_html5_video())\n",
    "\n",
    "create_animation(all_steps, iris_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
